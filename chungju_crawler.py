#!/usr/bin/env python
# coding: utf-8

# In[1]:


from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, UnexpectedAlertPresentException
from bs4 import BeautifulSoup
import time
import os

# âœ… ê²Œì‹œíŒ ëª©ë¡ì—ì„œ nttNo ìˆ˜ì§‘
def get_nttNo_list_fixed(max_count=4000):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--disable-gpu")
    options.add_argument("--log-level=3")
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)
    nttNo_list = []
    page = 1
    print("ğŸ“„ ëª©ë¡ í˜ì´ì§€ì—ì„œ nttNo ìˆ˜ì§‘ ì‹œì‘...")

    while len(nttNo_list) < max_count:
        url = f"https://www.chungju.go.kr/www/selectBbsNttList.do?key=494&bbsNo=6&pageIndex={page}"
        driver.get(url)

        print(f"ğŸ“„ í˜„ì¬ {page}í˜ì´ì§€ ì²˜ë¦¬ ì¤‘")
        print(f"ğŸ”¢ í˜„ì¬ê¹Œì§€ ìˆ˜ì§‘ëœ nttNo ê°œìˆ˜: {len(nttNo_list)}")

        try:
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "ul.list li"))
            )
        except TimeoutException:
            print(f"âš ï¸ ëª©ë¡ {page}í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
            break

        soup = BeautifulSoup(driver.page_source, "html.parser")
        links = soup.select("ul.list li a")

        for link in links:
            href = link.get("href")
            if "nttNo=" in href:
                ntt = href.split("nttNo=")[-1].split("&")[0]
                if ntt not in nttNo_list:
                    nttNo_list.append(ntt)
                    if len(nttNo_list) >= max_count:
                        break

        page += 1
        time.sleep(0.8)

    driver.quit()
    print(f"âœ… ìˆ˜ì§‘ëœ ê²Œì‹œê¸€ ìˆ˜: {len(nttNo_list)}ê°œ")
    return nttNo_list


# âœ… ê° nttNoë³„ ë³¸ë¬¸ í¬ë¡¤ë§
def crawl_articles_from_nttNos(nttNo_list, output_path="data/corpus.txt"):
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")
    options.add_argument("--ignore-certificate-errors")
    options.add_argument("--disable-gpu")
    options.add_argument("--log-level=3")
    options.add_argument("user-agent=Mozilla/5.0")

    driver = webdriver.Chrome(options=options)

    results = []
    print("ğŸš€ ê²Œì‹œê¸€ ë³¸ë¬¸ í¬ë¡¤ë§ ì‹œì‘")

    for idx, nttNo in enumerate(nttNo_list):
        url = f"https://www.chungju.go.kr/www/selectBbsNttView.do?key=494&bbsNo=6&nttNo={nttNo}"
        driver.get(url)

        try:
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "tr.code_SJ td"))
            )
        except (TimeoutException, UnexpectedAlertPresentException):
            try:
                alert = driver.switch_to.alert
                alert.dismiss()
            except:
                pass
            continue

        soup = BeautifulSoup(driver.page_source, "html.parser")
        title_tag = soup.select_one("tr.code_SJ td")
        date_tag = soup.select_one("tr.code_RGSDE td")
        content_tag = soup.select_one("tr.code_CN td.bbs_content")

        if title_tag and content_tag:
            title = title_tag.text.strip()
            date = date_tag.text.strip() if date_tag else "ë‚ ì§œ ì—†ìŒ"
            content = content_tag.get_text("\n", strip=True).replace('\xa0', ' ')
            article = f"[ì œëª©] {title}\n[ë‚ ì§œ] {date}\n[ë‚´ìš©]\n{content}\n---\n"
            results.append(article)

        # âœ… 100ê±´ë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
        if (idx + 1) % 100 == 0:
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            with open(output_path, "w", encoding="utf-8") as f:
                f.writelines(results)
            print(f"ğŸ’¾ {idx+1}ê±´ê¹Œì§€ ì €ì¥ ì™„ë£Œ")

        time.sleep(0.8)

    driver.quit()
    print(f"\nâœ… ì´ {len(results)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")

    # âœ… ìµœì¢… ì €ì¥
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        f.writelines(results)

    print(f"ğŸ’¾ ìµœì¢… ì €ì¥ ì™„ë£Œ â†’ {output_path}")


# âœ… ì‹¤í–‰
if __name__ == "__main__":
    ntt_list = get_nttNo_list_fixed(max_count=4000)
    crawl_articles_from_nttNos(ntt_list, output_path="data/corpus.txt")

